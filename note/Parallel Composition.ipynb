{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elia/Software/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from os.path import dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "note_dir = os.getcwd()\n",
    "root_dir = dirname(note_dir)\n",
    "data_dir = os.path.join(root_dir, 'resc', 'data', 'tidy','nltcs')\n",
    "\n",
    "# Filter relevant stuff out\n",
    "rel_fnames = [os.path.join(data_dir, f) for f in os.listdir(data_dir)\n",
    "              if 'F00' in f\n",
    "              if 'bayesfusion' not in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_fn, test_fn = rel_fnames[0], rel_fnames[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Get dfs\n",
    "df_train, df_test = pd.read_csv(train_fn, header=None), pd.read_csv(test_fn, header=None)\n",
    "\n",
    "# Get np.arrays\n",
    "train, test = df_train.values, df_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train some models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_rows, nb_atts = train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_codes = np.eye(nb_atts, dtype=np.int64)\n",
    "\n",
    "# Add second target\n",
    "for i in range(m_codes.shape[0]-1):\n",
    "    m_codes[i, i+1]=1\n",
    "\n",
    "m_codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Induction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_model(m_code, train, **kwargs):\n",
    "    desc_ids, targ_ids = np.where(m_code==0)[0], np.where(m_code==1)[0]\n",
    "    X, Y = train[:, desc_ids], train[:, targ_ids]\n",
    "    \n",
    "    clf = RandomForestClassifier(**kwargs)\n",
    "    clf.desc_ids = desc_ids\n",
    "    clf.targ_ids = targ_ids\n",
    "    \n",
    "    if X.shape[1]==1: X = X.ravel()\n",
    "    if Y.shape[1]==1: Y = Y.ravel()\n",
    "\n",
    "    clf.fit(X,Y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_list = []\n",
    "for m_idx, m_code in enumerate(m_codes):\n",
    "    m_list.append(learn_model(m_code, train, max_depth=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_targ = np.array([1])\n",
    "q_desc = np.arange(6,16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So model 0 and 1 can predict our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rel_models = m_list[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, Y_true = test[:, q_desc], test[:, q_targ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Composition\n",
    "\n",
    "A new, powerful way of combining models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MonoModel(object):\n",
    "    def fit(X, **kwargs):\n",
    "        return\n",
    "    \n",
    "    def predict(X, **kwargs):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "code_folding": [
     2,
     17,
     30,
     32,
     55,
     59,
     63,
     68,
     81,
     86,
     101,
     116,
     125,
     136,
     145,
     160
    ]
   },
   "outputs": [],
   "source": [
    "class ParallelComposition(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.estimators = []\n",
    "        \n",
    "        self.desc_ids = np.array([])\n",
    "        self.targ_ids = np.array([])\n",
    "        \n",
    "        self.classes_ = [np.array([])]\n",
    "        \n",
    "        self.n_classes_ = 0\n",
    "        self.n_outputs_ = 0\n",
    "        self.n_features_ = 0\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def fit(X, Y, **kwargs):\n",
    "        return\n",
    "    \n",
    "    def predict_proba(X, **kwargs):\n",
    "        nb_rows, nb_atts = X.shape\n",
    "        \n",
    "        s_proba = [np.zeros(nb_rows, n) for n in self.n_classes_]\n",
    "        \n",
    "        for e in self.estimators_:\n",
    "            e_proba = self._predict_proba_estimator(e, X, **kwargs)\n",
    "            s_proba = self._add_proba_estimator(e, e_proba, s_proba)\n",
    "        \n",
    "        # redo sklearn convention from hell\n",
    "        if len(s_proba) == 1:\n",
    "            return s_proba[0]\n",
    "        else:\n",
    "            return s_proba\n",
    "    \n",
    "    def predict(X, **kwargs):\n",
    "        nb_rows, nb_atts = X.shape\n",
    "        \n",
    "        s_pred = np.zeros(nb_rows, self.n_outputs)\n",
    "        \n",
    "        # redo sklearn convention from hell\n",
    "        if s_pred.shape[1] == 1:\n",
    "            return s_pred.ravel()\n",
    "        else:\n",
    "            return s_pred\n",
    "\n",
    "    # Updates (i.e., recalculate)\n",
    "    def _update_classes_(self):\n",
    "        # Re-initialize (easier)\n",
    "        self.classes_ = [np.array([])] * len(self.targ_ids)    \n",
    "        \n",
    "        for e in self.estimators_:\n",
    "            self._add_classes_estimator(e)\n",
    "        return\n",
    "    \n",
    "    def _update_n_classes_(self):\n",
    "        self.n_classes_ = [len(c) for c in self.classes_]\n",
    "        return \n",
    "    \n",
    "    def _update_n_outputs_(self):\n",
    "        self.n_outputs_ = len(self.targ_ids)\n",
    "        return \n",
    "    \n",
    "    def _update_n_features_(self):\n",
    "        self.n_features = len(self.desc_ids)\n",
    "        return \n",
    "    \n",
    "    # Add (i.e., incremental update)\n",
    "    def add_estimator(self, e):\n",
    "        self.estimators_.append(e)\n",
    "        \n",
    "        self._add_ids_estimator(e)\n",
    "\n",
    "        self._update_classes()\n",
    "        \n",
    "        self._update_n_classes_()\n",
    "        self._update_n_outputs_()\n",
    "        self._update_n_features_()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def _add_ids_estimator(self, e):\n",
    "        self.desc_ids = np.unique(np.concatenate((self.desc_ids, e.desc_ids)))\n",
    "        self.targ_ids = np.unique(np.concatenate((self.targ_ids, e.targ_ids)))\n",
    "        return\n",
    "    \n",
    "    def _add_classes_estimator(self, e):\n",
    "        \n",
    "        idx_map = self._map_elements_idx(e.targ_ids, self.targ_ids)\n",
    "        \n",
    "        def combine(classes_1, classes_2):\n",
    "            return np.unique(np.concatenate((classes_1, classes_2)))\n",
    "        \n",
    "        for idx_e, idx_s in idx_map:                    # `s` stands for `self`\n",
    "            e_classes_ = e.classes_[idx_e]\n",
    "            s_classes_ = self.classes_[idx_s]\n",
    "            \n",
    "            self.classes_[idx_s] = combine(e_classes_, s_classes_)\n",
    "    \n",
    "        return\n",
    "    \n",
    "    def _add_proba_estimator(self, e, e_proba, s_proba):\n",
    "        \n",
    "        t_idx_map = self._map_elements_idx(e.targ_ids, self.targ_ids)\n",
    "        \n",
    "        for t_idx_e, t_idx_s in idx_map:                    # `s` stands for `self`\n",
    "            l_idx_map = self._map_elements_idx(e.classes_[t_idx_e], self.classes_[t_idx_s])\n",
    "            l_idx_map = np.array(l_idx_map)\n",
    "            \n",
    "            l_idx_e, l_idx_s = l_idx_map[:,0], l_idx_map[:,1]\n",
    "            \n",
    "            s_proba[idx_s][:, l_idx_s] += e_proba[t_idx_e][:, l_idx_e]\n",
    "            \n",
    "        return s_proba\n",
    "    \n",
    "    # Estimator - utilities\n",
    "    def _predict_estimator_tidy(self, e, X, **kwargs):\n",
    "        \"\"\"\n",
    "        Ensure matrix.\n",
    "        \"\"\"\n",
    "        e_pred = e.predict(X, **kwargs)\n",
    "        \n",
    "        # undo sklearn convention from hell\n",
    "        return np.atleast_2d(e_pred)\n",
    "    \n",
    "    def _predict_proba_estimator_tidy(self, e, X, **kwargs):\n",
    "        \"\"\"\n",
    "        Ensure it is returned as a list.\n",
    "        \"\"\"\n",
    "        e_proba = e.predict_proba(X, **kwargs)\n",
    "        \n",
    "        # undo sklearn convention from hell\n",
    "        if isinstance(e_proba, np.ndarray):\n",
    "            return [e_proba]\n",
    "        elif isinstance(e_proba, list):\n",
    "            return e_proba\n",
    "        else:\n",
    "            msg = \"\"\"\n",
    "            e_proba has to be {np.ndarray, list},\n",
    "            instead the type was:   {}\n",
    "            \"\"\".format(type(e_proba))\n",
    "            raise TypeError(msg)\n",
    "            \n",
    "    \n",
    "    # Random utilities\n",
    "    def _map_elements_idx(self, a1, a2):\n",
    "        \"\"\"\n",
    "        Create a map that connects elements that occur in both arrays.\n",
    "\n",
    "        The output is a tuple list, with a tuple being;\n",
    "            (index of element e in a1, index of element e in a2)\n",
    "\n",
    "        N.b.:   Does not crash in case of double entries (behaviour is still correct),\n",
    "                but there are some ambiguities involved. I.e., do not do this.\n",
    "        \"\"\"\n",
    "        idx_a1 = np.where(np.in1d(a1, a2))[0]\n",
    "        idx_a2 = np.where(np.in1d(a2, a1))[0]\n",
    "\n",
    "        return list(zip(idx_a1, idx_a2))\n",
    "    \n",
    "    def filter_matrix(matrix, ids_1, ids_2):\n",
    "        \n",
    "        idx_map = np.array(self._map_elements_idx(ids_1, ids_2))\n",
    "        relevant_idx = idx_map[:, 0]\n",
    "        \n",
    "        if isinstance(matrix, np.ndarray):\n",
    "            # Case 1: Outcome of predict_proba of a single targets\n",
    "            # Case 2: Outcome of predict\n",
    "            return matrix[:, relevant_idx]\n",
    "        elif isinstance(matrix, list):\n",
    "            # Case 1: Outcome of predict_proba of a multiple targets\n",
    "            return [c for idx, c in enumerate(matrix) if idx in relevant_idx]\n",
    "        else:\n",
    "            msg = \"\"\"\n",
    "            Matrix has to be {np.ndarray, list},\n",
    "            instead the type was:   {}\n",
    "            \"\"\".format(type(matrix))\n",
    "            raise TypeError(msg)\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.atleast_2d(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _map_elements_idx(self, a1, a2):\n",
    "    \"\"\"\n",
    "    Create a map that connects elements that occur in both arrays.\n",
    "\n",
    "    The output is a tuple list, with a tuple being;\n",
    "        (index of element e in a1, index of element e in a2)\n",
    "\n",
    "    N.b.:   Does not crash in case of double entries (behaviour is still correct),\n",
    "            but there are some ambiguities involved. I.e., do not do this.\n",
    "    \"\"\"\n",
    "    idx_a1 = np.where(np.in1d(a1, a2))[0]\n",
    "    idx_a2 = np.where(np.in1d(a2, a1))[0]\n",
    "\n",
    "    return list(zip(idx_a1, idx_a2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_map_elements_idx(3, [1,2], [2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_output(f, in_ids, out_ids):\n",
    "    \n",
    "    idx_map = _map_elements_idx(3, [1,2], [2])\n",
    "    \n",
    "    array = f()\n",
    "    \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_two():\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alter_function(f):\n",
    "    return f() + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alter_function(get_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x[0] = np.array([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.append(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "code_folding": [
     11,
     29,
     35,
     54
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ParallelComposition(object):\n",
    "    \n",
    "    def __init__(self, estimators, targ_ids=None):\n",
    "        \n",
    "        # Estimators\n",
    "        self.estimators_ = estimators\n",
    "        \n",
    "        # Bookkeeping desc_ids/targ_ids\n",
    "        self.desc_ids = np.unique(np.concatenate([e.desc_ids for e in estimators]))\n",
    "        self.targ_ids = np.unique(np.concatenate([e.targ_ids for e in estimators]))\n",
    "        \n",
    "        if targ_ids is not None:\n",
    "            assert np.intersect1d(self.targ_ids, targ_ids).shape[0] > 0\n",
    "            self.targ_ids = targ_ids\n",
    "        \n",
    "        # Bookkeeping classes_\n",
    "        self.classes_ = [[]] * len(self.targ_ids)    # Init\n",
    "    \n",
    "        for e in self.estimators_:\n",
    "            self._add_classes_estimator(e)\n",
    "          \n",
    "        # n_classes_/n_outputs_/n_features_\n",
    "        self.n_classes_ = self.get_n_classes_()\n",
    "        self.n_outputs_ = self.get_n_outputs_() \n",
    "        self.n_features_ = self.get_n_features_()\n",
    "        \n",
    "        \n",
    "        return\n",
    "    \n",
    "    def get_n_classes_(self):\n",
    "        return [len(c) for c in self.classes_]\n",
    "    \n",
    "    def get_n_outputs_(self):\n",
    "        return len(self.targ_ids)\n",
    "    \n",
    "    def gen_n_features_(self):\n",
    "        return len(self.desc_ids)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _add_proba_estimator(self, e, e_proba, s_proba):\n",
    "        \n",
    "        t_idx_map = self._map_elements_idx(e.targ_ids, self.targ_ids)\n",
    "        \n",
    "        for t_idx_e, t_idx_s in idx_map:                    # `s` stands for `self`\n",
    "            l_idx_map = self._map_elements_idx(e.classes_[t_idx_e], self.classes_[t_idx_s])\n",
    "            l_idx_map = np.array(l_idx_map)\n",
    "            \n",
    "            l_idx_e, l_idx_s = l_idx_map[:,0], l_idx_map[:,1]\n",
    "            \n",
    "            s_proba[idx_s][:, l_idx_e] += e_proba[t_idx_e][:, l_idx_s]\n",
    "            \n",
    "        return s_proba\n",
    "    \n",
    "    def _add_classes_estimator(self, e):\n",
    "        \n",
    "        idx_map = self._map_elements_idx(e.targ_ids, self.targ_ids)\n",
    "        \n",
    "        def combine(classes_1, classes_2):\n",
    "            return np.unique(np.concatenate((classes_1, classes_2)))\n",
    "        \n",
    "        for idx_e, idx_s in idx_map:                    # `s` stands for `self`\n",
    "            e_classes_ = e.classes_[idx_e]\n",
    "            s_classes_ = self.classes_[idx_s]\n",
    "            \n",
    "            self.classes_[idx_s] = combine(e_classes_, s_classes_)\n",
    "    \n",
    "        return\n",
    "    \n",
    "    def _map_elements_idx(self, a1, a2):\n",
    "        \"\"\"\n",
    "        Create a map that connects elements that occur in both arrays.\n",
    "\n",
    "        The output is a tuple list, with a tuple being;\n",
    "            (index of element e in a1, index of element e in a2)\n",
    "\n",
    "        N.b.:   Does not crash in case of double entries (behaviour is still correct),\n",
    "                but there are some ambiguities involved. I.e., do not do this.\n",
    "        \"\"\"\n",
    "        idx_a1 = np.where(np.in1d(a1, a2))[0]\n",
    "        idx_a2 = np.where(np.in1d(a2, a1))[0]\n",
    "\n",
    "        return list(zip(idx_a1, idx_a2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numeric_part = np.random.rand(40, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.65812399, 0.        , 0.34973964],\n",
       "       [0.60505119, 0.        , 0.24788577],\n",
       "       [0.24986698, 0.        , 0.96523949],\n",
       "       [0.71304659, 0.        , 0.95414958],\n",
       "       [0.93727158, 0.        , 0.4643154 ],\n",
       "       [0.3777205 , 0.        , 0.39411408],\n",
       "       [0.41942129, 0.        , 0.07825372],\n",
       "       [0.91659915, 0.        , 0.11977484],\n",
       "       [0.98139557, 0.        , 0.03147218],\n",
       "       [0.68478324, 0.        , 0.8274138 ],\n",
       "       [0.05019347, 0.        , 0.75555512],\n",
       "       [0.07708851, 0.        , 0.75367142],\n",
       "       [0.42020199, 0.        , 0.5909857 ],\n",
       "       [0.12901189, 0.        , 0.0413294 ],\n",
       "       [0.77193544, 0.        , 0.22257293],\n",
       "       [0.46264678, 0.        , 0.56154757],\n",
       "       [0.11163773, 0.        , 0.09509286],\n",
       "       [0.44868599, 0.        , 0.03398905],\n",
       "       [0.01098235, 0.        , 0.18857854],\n",
       "       [0.61451563, 0.        , 0.13701001],\n",
       "       [0.5597041 , 0.        , 0.62846366],\n",
       "       [0.6328595 , 0.        , 0.25576423],\n",
       "       [0.39859294, 0.        , 0.40052084],\n",
       "       [0.44839092, 0.        , 0.92752219],\n",
       "       [0.80036807, 0.        , 0.08293705],\n",
       "       [0.57283381, 0.        , 0.66769249],\n",
       "       [0.02286602, 0.        , 0.99162516],\n",
       "       [0.73232493, 0.        , 0.64740229],\n",
       "       [0.59272907, 0.        , 0.63190541],\n",
       "       [0.86390342, 0.        , 0.92928285],\n",
       "       [0.07313821, 0.        , 0.74877904],\n",
       "       [0.73494958, 0.        , 0.49662528],\n",
       "       [0.4632623 , 0.        , 0.68307081],\n",
       "       [0.68493791, 0.        , 0.14747087],\n",
       "       [0.08564975, 0.        , 0.1693287 ],\n",
       "       [0.02189924, 0.        , 0.71797422],\n",
       "       [0.52234315, 0.        , 0.28862725],\n",
       "       [0.81994998, 0.        , 0.25532419],\n",
       "       [0.96762825, 0.        , 0.18141521],\n",
       "       [0.58228453, 0.        , 0.29076561]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = np.zeros((40,3))\n",
    "template[:, [0,2]] = numeric_part\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric_part = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nominal_part = np.zeros((40,2))\n",
    "nominal_part[:, [0]] = np.random.randint(0,2,size=(40,1))\n",
    "nominal_part[:, [1]] = np.random.randint(1,4,size=(40,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.45450299, 0.07400197, 0.        , 2.        ],\n",
       "       [0.05916802, 0.43626181, 0.        , 2.        ],\n",
       "       [0.68352255, 0.75790171, 1.        , 3.        ],\n",
       "       [0.16816202, 0.86355601, 0.        , 2.        ],\n",
       "       [0.45089349, 0.21092979, 0.        , 1.        ],\n",
       "       [0.24297159, 0.86075446, 0.        , 2.        ],\n",
       "       [0.55277581, 0.35687691, 1.        , 3.        ],\n",
       "       [0.52052585, 0.47755787, 1.        , 3.        ],\n",
       "       [0.52395587, 0.75148139, 1.        , 2.        ],\n",
       "       [0.86052228, 0.54671914, 1.        , 3.        ],\n",
       "       [0.37622751, 0.21844369, 1.        , 2.        ],\n",
       "       [0.17595598, 0.86009243, 0.        , 1.        ],\n",
       "       [0.0921844 , 0.76463247, 1.        , 1.        ],\n",
       "       [0.0690307 , 0.72279878, 0.        , 1.        ],\n",
       "       [0.41149388, 0.78493443, 0.        , 2.        ],\n",
       "       [0.31378192, 0.7018987 , 0.        , 2.        ],\n",
       "       [0.26668953, 0.59423633, 0.        , 3.        ],\n",
       "       [0.03155495, 0.61533786, 0.        , 2.        ],\n",
       "       [0.62742665, 0.59056072, 1.        , 2.        ],\n",
       "       [0.55437462, 0.82155784, 1.        , 2.        ],\n",
       "       [0.49231767, 0.90476587, 1.        , 2.        ],\n",
       "       [0.6385371 , 0.67723223, 1.        , 1.        ],\n",
       "       [0.21526577, 0.56504204, 0.        , 1.        ],\n",
       "       [0.91511745, 0.97283396, 1.        , 3.        ],\n",
       "       [0.58258868, 0.05538846, 0.        , 2.        ],\n",
       "       [0.06513343, 0.38663495, 0.        , 3.        ],\n",
       "       [0.50233902, 0.38290034, 0.        , 1.        ],\n",
       "       [0.9285596 , 0.70273152, 1.        , 3.        ],\n",
       "       [0.42071073, 0.98497694, 0.        , 3.        ],\n",
       "       [0.54028183, 0.86498338, 1.        , 2.        ],\n",
       "       [0.14609516, 0.97621174, 1.        , 2.        ],\n",
       "       [0.31650742, 0.46506671, 1.        , 1.        ],\n",
       "       [0.50529251, 0.71559662, 1.        , 3.        ],\n",
       "       [0.697976  , 0.91195678, 0.        , 1.        ],\n",
       "       [0.85601653, 0.76820385, 1.        , 2.        ],\n",
       "       [0.58252217, 0.07449833, 0.        , 2.        ],\n",
       "       [0.59902391, 0.82498759, 1.        , 2.        ],\n",
       "       [0.96058003, 0.69096633, 1.        , 1.        ],\n",
       "       [0.3656803 , 0.51925674, 1.        , 1.        ],\n",
       "       [0.37582498, 0.46524462, 0.        , 3.        ]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = np.zeros((40,4))\n",
    "dataset[:, 0:2] = numeric_part\n",
    "dataset[:, 2:] = nominal_part\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = ParallelComposition(rel_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2, 2]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.n_classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = [(1,2),(3,4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tl)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.targ_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 1.]), array([0., 1.]), array([0., 1.])]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 2]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.n_classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
